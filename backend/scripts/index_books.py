#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ PDF-–∫–Ω–∏–≥ –¥–ª—è AI –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏.

–ò–ù–°–¢–†–£–ö–¶–ò–Ø –ü–û –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ:

1. –ü–æ–ª–æ–∂–∏—Ç–µ –≤—Å–µ PDF-–∫–Ω–∏–≥–∏ –ø–æ –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏–∏ –≤ –ø–∞–ø–∫—É:
   backend/app/data/books/

2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ .env —Ñ–∞–π–ª–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω OPENAI_API_KEY:
   OPENAI_API_KEY=your_api_key_here
   EMBEDDING_MODEL=text-embedding-3-small  (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

3. –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
   cd backend
   source .venv/bin/activate  # –∏–ª–∏ –¥—Ä—É–≥–æ–π –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É venv

4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç:
   python -m scripts.index_books
   # –∏–ª–∏
   python scripts/index_books.py

5. –†–µ–∑—É–ª—å—Ç–∞—Ç:
   - –°–∫—Ä–∏–ø—Ç –æ–±—Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ PDF-—Ñ–∞–π–ª—ã –∏–∑ –ø–∞–ø–∫–∏ books/
   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ PDF –∏–∑–≤–ª–µ—á—ë—Ç —Ç–µ–∫—Å—Ç –∏ —Ä–∞–∑–æ–±—å—ë—Ç –Ω–∞ —á–∞–Ω–∫–∏
   - –î–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ –ø–æ–ª—É—á–∏—Ç embedding —á–µ—Ä–µ–∑ OpenAI
   - –°–æ—Ö—Ä–∞–Ω–∏—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ backend/app/data/ai_knowledge/chunks.json

6. –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.

–í–ù–ò–ú–ê–ù–ò–ï:
- –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏ —Ä–∞–∑–º–µ—Ä–∞ –∫–Ω–∏–≥)
- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è API OpenAI, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –±–∞–ª–∞–Ω—Å
- –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –∑–∞–ø—É—Å–∫–µ chunks.json –±—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω
"""
import sys
import os
from pathlib import Path
import json
import time
from typing import List, Dict

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ app –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import pdfplumber
except ImportError:
    print("–û–®–ò–ë–ö–ê: pdfplumber –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pdfplumber")
    sys.exit(1)

try:
    from app.openai_client import get_embedding
except ImportError as e:
    print(f"–û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å openai_client: {e}")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ OPENAI_API_KEY —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ .env")
    sys.exit(1)


def extract_text_from_pdf(pdf_path: Path) -> List[Dict[str, any]]:
    """
    –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ PDF –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ.
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π: [{"page": int, "text": str}, ...]
    """
    pages_data = []
    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, start=1):
                text = page.extract_text()
                if text:
                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
                    text = " ".join(text.split())  # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
                    pages_data.append({
                        "page": page_num,
                        "text": text
                    })
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF {pdf_path.name}: {e}")
        return []
    
    return pages_data


def split_into_chunks(text: str, chunk_size: int = 800, overlap: int = 100) -> List[str]:
    """
    –†–∞–∑–±–∏—Ç—å —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫-—á–∞–Ω–∫–æ–≤
    """
    if len(text) <= chunk_size:
        return [text] if text.strip() else []
    
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –≥—Ä–∞–Ω–∏—Ü–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        if end < len(text):
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–æ—á–∫—É, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∑–Ω–∞–∫
            for i in range(end, max(start, end - 200), -1):
                if text[i] in '.!?':
                    end = i + 1
                    break
        
        chunk = text[start:end].strip()
        if len(chunk) >= 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            chunks.append(chunk)
        
        start = end - overlap
        if start >= len(text):
            break
    
    return chunks


def process_pdf(pdf_path: Path, chunk_id_start: int) -> tuple[List[Dict], int]:
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–∏–Ω PDF —Ñ–∞–π–ª.
    
    Returns:
        (—Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤, —Å–ª–µ–¥—É—é—â–∏–π –¥–æ—Å—Ç—É–ø–Ω—ã–π ID)
    """
    print(f"\nüìñ –û–±—Ä–∞–±–æ—Ç–∫–∞: {pdf_path.name}")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ
    pages_data = extract_text_from_pdf(pdf_path)
    if not pages_data:
        print(f"  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {pdf_path.name}")
        return [], chunk_id_start
    
    print(f"  üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {len(pages_data)}")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    all_chunks = []
    for page_data in pages_data:
        page_num = page_data["page"]
        text = page_data["text"]
        chunks = split_into_chunks(text)
        
        for offset, chunk_text in enumerate(chunks, start=1):
            all_chunks.append({
                "page": page_num,
                "offset": offset,
                "text": chunk_text
            })
    
    print(f"  ‚úÇÔ∏è  –°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    
    # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞
    chunks_with_embeddings = []
    for i, chunk in enumerate(all_chunks, 1):
        try:
            print(f"  üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i}/{len(all_chunks)}...", end="\r")
            embedding = get_embedding(chunk["text"])
            
            chunk_data = {
                "id": chunk_id_start + i - 1,
                "book": pdf_path.stem,  # –ò–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                "page": chunk["page"],
                "offset": chunk["offset"],
                "text": chunk["text"],
                "embedding": embedding
            }
            chunks_with_embeddings.append(chunk_data)
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limits
            time.sleep(0.1)
        except Exception as e:
            print(f"\n  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ embedding –¥–ª—è —á–∞–Ω–∫–∞ {i}: {e}")
            continue
    
    print(f"  ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks_with_embeddings)}")
    
    return chunks_with_embeddings, chunk_id_start + len(all_chunks)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞."""
    print("=" * 60)
    print("üìö –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ö–ù–ò–ì –î–õ–Ø AI –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ò")
    print("=" * 60)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç–∏
    backend_dir = Path(__file__).parent.parent
    books_dir = backend_dir / "app" / "data" / "books"
    output_file = backend_dir / "app" / "data" / "ai_knowledge" / "chunks.json"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É —Å –∫–Ω–∏–≥–∞–º–∏
    if not books_dir.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {books_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –°–æ–∑–¥–∞—é...")
        books_dir.mkdir(parents=True, exist_ok=True)
    
    # –ò—â–µ–º –≤—Å–µ PDF —Ñ–∞–π–ª—ã
    pdf_files = list(books_dir.glob("*.pdf"))
    
    if not pdf_files:
        print(f"\n‚ö†Ô∏è  –í –ø–∞–ø–∫–µ {books_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ PDF-—Ñ–∞–π–ª–æ–≤.")
        print("   –ü–æ–ª–æ–∂–∏—Ç–µ PDF-–∫–Ω–∏–≥–∏ –≤ —ç—Ç—É –ø–∞–ø–∫—É –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞.")
        return
    
    print(f"\nüìÅ –ù–∞–π–¥–µ–Ω–æ PDF-—Ñ–∞–π–ª–æ–≤: {len(pdf_files)}")
    for pdf in pdf_files:
        print(f"   - {pdf.name}")
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ PDF
    all_chunks = []
    chunk_id = 1
    start_time = time.time()
    
    for pdf_path in pdf_files:
        chunks, chunk_id = process_pdf(pdf_path, chunk_id)
        all_chunks.extend(chunks)
    
    elapsed_time = time.time() - start_time
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ {output_file}...")
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)
    
    print(f"\n‚úÖ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"   üìä –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(all_chunks)}")
    print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è: {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"   üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_file}")
    print("\n–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—é –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏!")


if __name__ == "__main__":
    main()


